#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
model_egitici.py
----------------
Makine Ã¶ÄŸrenmesi modelleri iÃ§in birleÅŸtirilmiÅŸ eÄŸitim ve deÄŸerlendirme modÃ¼lÃ¼.

Bu dosya, MRI sÄ±nÄ±flandÄ±rma projesi iÃ§in 3 farklÄ± model seÃ§eneÄŸi sunar:
1. XGBoost (Gradient Boosting) - Ã–nerilen, yÃ¼ksek performans
2. LightGBM (Gradient Boosting) - HÄ±zlÄ±, bÃ¼yÃ¼k veri setleri iÃ§in
3. Linear SVM - Basit, hÄ±zlÄ± eÄŸitim

MODEL SEÃ‡Ä°MÄ°:
- Model tipi, program baÅŸlatÄ±ldÄ±ÄŸÄ±nda kullanÄ±cÄ±ya sorulur
- Veya kod iÃ§inde ModelEgitici(model_tipi="xgboost") ile belirlenebilir
- Ayarlar model/ayarlar.py dosyasÄ±nda yapÄ±landÄ±rÄ±lÄ±r
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import numpy as np
import pandas as pd
import pickle
import json
from typing import Dict, Any, Optional, Tuple, List
from datetime import datetime

# Scikit-learn modÃ¼lleri
from sklearn.model_selection import (
    train_test_split, GridSearchCV, RandomizedSearchCV,
    StratifiedKFold, cross_val_score
)
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score,
    cohen_kappa_score
)
from sklearn.feature_selection import SelectKBest, mutual_info_classif

# Imbalanced-learn (SMOTE iÃ§in)
try:
    from imblearn.over_sampling import SMOTE
    SMOTE_AVAILABLE = True
except ImportError:
    SMOTE_AVAILABLE = False
    print("[UYARI] imbalanced-learn yÃ¼klÃ¼ deÄŸil. SMOTE kullanÄ±lamayacak.")

import matplotlib.pyplot as plt
import seaborn as sns

from ayarlar import *

# Ensure VERI_CSV is imported
try:
    from ayarlar import VERI_CSV
except ImportError:
    VERI_CSV = Path("goruntu_isleme/cikti/goruntu_ozellikleri_scaled.csv")


class ModelEgitici:
    """
    TÃ¼m model eÄŸitim ve deÄŸerlendirme iÅŸlemleri iÃ§in birleÅŸik sÄ±nÄ±f.
    
    Bu sÄ±nÄ±f, 3 farklÄ± makine Ã¶ÄŸrenmesi algoritmasÄ±nÄ± destekler:
    - XGBoost: Gradient boosting, yÃ¼ksek performans
    - LightGBM: Gradient boosting, hÄ±zlÄ± eÄŸitim
    - Linear SVM: DoÄŸrusal sÄ±nÄ±flandÄ±rÄ±cÄ±, basit ve hÄ±zlÄ±
    """
    
    def __init__(self, model_tipi: str = "xgboost", 
                 smote_aktif: bool = True,
                 feature_selection_aktif: bool = False):
        """
        Model eÄŸiticiyi baÅŸlat.
        
        Args:
            model_tipi: EÄŸitilecek model tÃ¼rÃ¼
                - "xgboost": XGBoost gradient boosting (Ã¶nerilen)
                - "lightgbm": LightGBM gradient boosting (hÄ±zlÄ±)
                - "svm": Linear Support Vector Machine (basit)
            smote_aktif: SMOTE ile veri dengeleme yapÄ±lsÄ±n mÄ±?
            feature_selection_aktif: Ã–zellik seÃ§imi yapÄ±lsÄ±n mÄ±?
        """
        self.model_tipi = model_tipi
        self.model = None
        self.feature_names = None
        self.selected_features = None
        self.metrikler = {}
        self.smote_aktif = smote_aktif and SMOTE_AVAILABLE
        self.feature_selection_aktif = feature_selection_aktif
        self.cv_scores = None
        
        # Ã‡Ä±ktÄ± klasÃ¶rlerini oluÅŸtur
        print(f"\nğŸ“ Ã‡Ä±ktÄ± klasÃ¶rleri oluÅŸturuluyor...")
        for klasor in [CIKTI_KLASORU, MODELS_KLASORU, RAPORLAR_KLASORU, GORSELLER_KLASORU]:
            klasor.mkdir(parents=True, exist_ok=True)
        print(f"   âœ“ KlasÃ¶rler hazÄ±r: {CIKTI_KLASORU}")
    
    def veri_yukle(self, csv_yolu: Path = VERI_CSV) -> Tuple:
        """
        CSV dosyasÄ±ndan veri yÃ¼kle ve eÄŸitim/doÄŸrulama/test setlerine bÃ¶l.
        
        Args:
            csv_yolu: Ã–zellik CSV dosyasÄ±nÄ±n yolu
            
        Returns:
            Tuple: (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        print(f"\nğŸ“Š Veri yÃ¼kleniyor: {csv_yolu}")
        
        if not csv_yolu.exists():
            raise FileNotFoundError(
                f"âŒ CSV dosyasÄ± bulunamadÄ±: {csv_yolu}\n"
                f"   Ã–nce 'goruntu_isleme/ana_islem.py' Ã§alÄ±ÅŸtÄ±rarak CSV oluÅŸturun!"
            )
        
        # CSV'yi oku
        df = pd.read_csv(csv_yolu)
        print(f"   âœ“ {len(df)} kayÄ±t yÃ¼klendi")
        print(f"   âœ“ {df['sinif'].nunique()} sÄ±nÄ±f var: {df['sinif'].unique().tolist()}")
        
        # Ã–zellikler ve etiketler
        kategorik = ['dosya_adi', 'sinif', 'tam_yol']
        X = df.drop(columns=[c for c in kategorik if c in df.columns] + ['etiket'])
        y = df['etiket']
        
        self.feature_names = X.columns.tolist()
        print(f"   âœ“ {len(self.feature_names)} Ã¶zellik kullanÄ±lacak")
        
        # Ä°lk bÃ¶lme: eÄŸitim + geÃ§ici (doÄŸrulama + test)
        X_train, X_temp, y_train, y_temp = train_test_split(
            X, y,
            test_size=(1 - EGITIM_ORANI),
            random_state=RASTGELE_TOHUM,
            stratify=y if STRATIFY_AKTIF else None
        )
        
        # Ä°kinci bÃ¶lme: doÄŸrulama + test
        val_oran = DOGRULAMA_ORANI / (DOGRULAMA_ORANI + TEST_ORANI)
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp,
            test_size=(1 - val_oran),
            random_state=RASTGELE_TOHUM,
            stratify=y_temp if STRATIFY_AKTIF else None
        )
        
        print(f"\nğŸ“‚ Veri seti bÃ¶lÃ¼ndÃ¼:")
        print(f"   â€¢ EÄŸitim: {len(X_train)} ({len(X_train)/len(df)*100:.1f}%)")
        print(f"   â€¢ DoÄŸrulama: {len(X_val)} ({len(X_val)/len(df)*100:.1f}%)")
        print(f"   â€¢ Test: {len(X_test)} ({len(X_test)/len(df)*100:.1f}%)")
        
        # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
        print(f"\nğŸ“Š SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± (EÄŸitim seti):")
        for sinif, sayi in zip(*np.unique(y_train, return_counts=True)):
            print(f"   SÄ±nÄ±f {sinif}: {sayi} ({sayi/len(y_train)*100:.1f}%)")
        
        # SMOTE uygula (veri dengeleme)
        if self.smote_aktif:
            print(f"\nğŸ”„ SMOTE ile veri dengeleme yapÄ±lÄ±yor...")
            smote = SMOTE(random_state=RASTGELE_TOHUM, k_neighbors=3)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            print(f"   âœ“ SMOTE tamamlandÄ±. Yeni eÄŸitim seti: {len(X_train)} kayÄ±t")
            print(f"\nğŸ“Š Dengeli sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
            for sinif, sayi in zip(*np.unique(y_train, return_counts=True)):
                print(f"   SÄ±nÄ±f {sinif}: {sayi} ({sayi/len(y_train)*100:.1f}%)")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def feature_selection(self, X_train, y_train, k: int = 15):
        """
        En Ã¶nemli k Ã¶zelliÄŸi seÃ§ (mutual information kullanarak).
        
        Args:
            X_train: EÄŸitim Ã¶zellikleri
            y_train: EÄŸitim etiketleri
            k: SeÃ§ilecek Ã¶zellik sayÄ±sÄ±
            
        Returns:
            SeÃ§ilmiÅŸ Ã¶zellikler
        """
        if not self.feature_selection_aktif:
            return X_train
        
        print(f"\nğŸ” Feature Selection: En iyi {k} Ã¶zellik seÃ§iliyor...")
        
        # Mutual information ile Ã¶zellik skorlarÄ± hesapla
        selector = SelectKBest(score_func=mutual_info_classif, k=k)
        X_train_selected = selector.fit_transform(X_train, y_train)
        
        # SeÃ§ilen Ã¶zellikleri kaydet
        selected_indices = selector.get_support(indices=True)
        self.selected_features = [self.feature_names[i] for i in selected_indices]
        
        print(f"   âœ“ {len(self.selected_features)} Ã¶zellik seÃ§ildi:")
        scores = selector.scores_[selected_indices]
        for feat, score in sorted(zip(self.selected_features, scores), 
                                 key=lambda x: x[1], reverse=True)[:10]:
            print(f"      â€¢ {feat}: {score:.4f}")
        
        return X_train_selected
    
    def model_olustur(self):
        """
        SeÃ§ilen model tipine gÃ¶re ML modeli oluÅŸtur.
        
        3 farklÄ± model tipi desteklenir:
        
        1. XGBoost (xgboost):
           - Gradient boosting aÄŸaÃ§larÄ±
           - YÃ¼ksek doÄŸruluk, orta hÄ±z
           - Hiperparametre: n_estimators, max_depth, learning_rate, vb.
           - class_weight: Otomatik sÄ±nÄ±f dengeleme
        
        2. LightGBM (lightgbm):
           - Microsoft'un gradient boosting implementasyonu
           - Ã‡ok hÄ±zlÄ± eÄŸitim, bÃ¼yÃ¼k veri setleri iÃ§in ideal
           - Histogram tabanlÄ±, bellek verimli
           - class_weight: Otomatik sÄ±nÄ±f dengeleme
        
        3. Linear SVM (svm):
           - DoÄŸrusal Support Vector Machine
           - Basit, hÄ±zlÄ± eÄŸitim
           - class_weight: 'balanced' - otomatik aÄŸÄ±rlÄ±klandÄ±rma
        
        Returns:
            EÄŸitilmemiÅŸ model nesnesi (self.model'e atanÄ±r)
        """
        print(f"\nğŸ¤– Model oluÅŸturuluyor: {self.model_tipi.upper()}")
        
        if self.model_tipi == "xgboost":
            try:
                import xgboost as xgb
                # XGBoost modelini oluÅŸtur
                self.model = xgb.XGBClassifier(**GB_AYARLARI)
                print(f"   âœ“ XGBoost modeli hazÄ±r")
                print(f"   â„¹ï¸  n_estimators={GB_AYARLARI['n_estimators']}, max_depth={GB_AYARLARI['max_depth']}")
            except ImportError:
                raise ImportError(
                    "âŒ XGBoost yÃ¼klÃ¼ deÄŸil!\n"
                    "   Kurulum: pip install xgboost"
                )
        
        elif self.model_tipi == "lightgbm":
            try:
                import lightgbm as lgb
                # LightGBM parametrelerini XGBoost'tan dÃ¶nÃ¼ÅŸtÃ¼r
                lgb_params = GB_AYARLARI.copy()
                lgb_params['num_leaves'] = 2 ** GB_AYARLARI['max_depth']
                lgb_params.pop('max_depth', None)
                lgb_params.pop('early_stopping_rounds', None)
                
                self.model = lgb.LGBMClassifier(**lgb_params)
                print(f"   âœ“ LightGBM modeli hazÄ±r")
                print(f"   â„¹ï¸  n_estimators={lgb_params['n_estimators']}, num_leaves={lgb_params['num_leaves']}")
            except ImportError:
                raise ImportError(
                    "âŒ LightGBM yÃ¼klÃ¼ deÄŸil!\n"
                    "   Kurulum: pip install lightgbm"
                )
        
        elif self.model_tipi == "svm":
            from sklearn.svm import LinearSVC
            # Linear SVM modelini oluÅŸtur
            self.model = LinearSVC(**SVM_AYARLARI)
            print(f"   âœ“ Linear SVM modeli hazÄ±r")
            print(f"   â„¹ï¸  C={SVM_AYARLARI['C']}, class_weight={SVM_AYARLARI['class_weight']}")
        
        else:
            raise ValueError(
                f"âŒ Desteklenmeyen model tipi: {self.model_tipi}\n"
                f"   GeÃ§erli seÃ§enekler: 'xgboost', 'lightgbm', 'svm'"
            )
    
    def egit(self, X_train, y_train, X_val=None, y_val=None):
        """
        Modeli eÄŸit.
        
        Bu fonksiyon, hazÄ±rlanan veri seti ile makine Ã¶ÄŸrenmesi modelini eÄŸitir.
        
        EÄŸitim sÃ¼reci:
        1. Model oluÅŸturulur (henÃ¼z oluÅŸturulmadÄ±ysa)
        2. Early stopping iÃ§in doÄŸrulama seti kontrol edilir
        3. Model fit() metodu ile eÄŸitilir
        4. EÄŸitim sÃ¼resi Ã¶lÃ§Ã¼lÃ¼r
        
        Early Stopping (XGBoost/LightGBM iÃ§in):
        - DoÄŸrulama seti verilirse early stopping aktif olur
        - Model, doÄŸrulama kaybÄ± artmayÄ± bÄ±rakÄ±nca eÄŸitimi durdurur
        - Overfitting'i Ã¶nler ve eÄŸitim sÃ¼resini kÄ±saltÄ±r
        
        Args:
            X_train: EÄŸitim Ã¶zellikleri (features) - numpy array veya pandas DataFrame
            y_train: EÄŸitim etiketleri (labels) - numpy array veya pandas Series
            X_val: DoÄŸrulama Ã¶zellikleri (opsiyonel, early stopping iÃ§in gerekli)
            y_val: DoÄŸrulama etiketleri (opsiyonel, early stopping iÃ§in gerekli)
            
        Returns:
            None (model self.model'de saklanÄ±r)
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ MODEL EÄÄ°TÄ°MÄ° BAÅLIYOR: {self.model_tipi.upper()}")
        print(f"{'='*60}")
        
        if self.model is None:
            self.model_olustur()
        
        # EÄŸitim baÅŸlat
        print(f"\nâ³ EÄŸitim devam ediyor...")
        
        if self.model_tipi in ["xgboost", "lightgbm"] and X_val is not None:
            # Gradient boosting iÃ§in early stopping kullan
            if self.model_tipi == "xgboost":
                self.model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    verbose=False
                )
            else:  # lightgbm
                # LightGBM 'verbose' yerine 'log_evaluation' kullanÄ±r
                self.model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)]
                )
            print(f"   âœ“ Early stopping ile eÄŸitim tamamlandÄ±")
        else:
            self.model.fit(X_train, y_train)
            print(f"   âœ“ EÄŸitim tamamlandÄ±")
    
    def tahmin_yap(self, X):
        """Tahmin yap."""
        if self.model is None:
            raise ValueError("âŒ Model henÃ¼z eÄŸitilmemiÅŸ!")
        return self.model.predict(X)
    
    def degerlendir(self, X, y, set_adi: str = "Test") -> Dict:
        """
        Model performansÄ±nÄ± deÄŸerlendir.
        
        Args:
            X: Ã–zellikler
            y: GerÃ§ek etiketler
            set_adi: Veri seti adÄ± (Test, DoÄŸrulama, vb.)
            
        Returns:
            Metrikler sÃ¶zlÃ¼ÄŸÃ¼
        """
        y_pred = self.tahmin_yap(X)
        
        metrikler = {
            'accuracy': accuracy_score(y, y_pred),
            'precision_macro': precision_score(y, y_pred, average='macro', zero_division=0),
            'recall_macro': recall_score(y, y_pred, average='macro', zero_division=0),
            'f1_macro': f1_score(y, y_pred, average='macro', zero_division=0),
            'confusion_matrix': confusion_matrix(y, y_pred),
            'classification_report': classification_report(y, y_pred, zero_division=0),
        }
        
        # Cohen's Kappa (sÄ±nÄ±f dengesizliÄŸine robust)
        metrikler['cohen_kappa'] = cohen_kappa_score(y, y_pred)
        
        # ROC-AUC (multi-class iÃ§in one-vs-rest)
        if hasattr(self.model, 'predict_proba'):
            try:
                y_prob = self.model.predict_proba(X)
                metrikler['roc_auc_ovr'] = roc_auc_score(
                    y, y_prob, 
                    multi_class='ovr', 
                    average='macro'
                )
            except Exception as e:
                metrikler['roc_auc_ovr'] = None
                print(f"   [UYARI] ROC-AUC hesaplanamadÄ±: {e}")
        else:
            metrikler['roc_auc_ovr'] = None
        
        # Ekrana yazdÄ±r
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {set_adi.upper()} SETÄ° PERFORMANSI")
        print(f"{'='*60}")
        print(f"âœ“ DoÄŸruluk (Accuracy):    {metrikler['accuracy']:.4f}")
        print(f"âœ“ Kesinlik (Precision):   {metrikler['precision_macro']:.4f}")
        print(f"âœ“ DuyarlÄ±lÄ±k (Recall):    {metrikler['recall_macro']:.4f}")
        print(f"âœ“ F1 Skoru:               {metrikler['f1_macro']:.4f}")
        print(f"âœ“ Cohen's Kappa:          {metrikler['cohen_kappa']:.4f}")
        if metrikler['roc_auc_ovr'] is not None:
            print(f"âœ“ ROC-AUC (OvR):          {metrikler['roc_auc_ovr']:.4f}")
        print(f"\nğŸ“‹ DetaylÄ± Rapor:\n{metrikler['classification_report']}")
        
        return metrikler
    
    def cross_validate(self, X, y, cv_folds: int = 5) -> Dict[str, List[float]]:
        """
        K-fold cross-validation ile model performansÄ±nÄ± deÄŸerlendir.
        
        Args:
            X: Ã–zellikler
            y: Etiketler
            cv_folds: Cross-validation fold sayÄ±sÄ±
            
        Returns:
            Her fold iÃ§in skorlar sÃ¶zlÃ¼ÄŸÃ¼
        """
        print(f"\nğŸ”„ {cv_folds}-Fold Cross-Validation yapÄ±lÄ±yor...")
        
        if self.model is None:
            self.model_olustur()
        
        # Stratified K-Fold (sÄ±nÄ±f oranlarÄ±nÄ± korur)
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RASTGELE_TOHUM)
        
        # FarklÄ± metriklerle skorla
        scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
        cv_results = {}
        
        for metric in scoring_metrics:
            scores = cross_val_score(self.model, X, y, cv=skf, scoring=metric, n_jobs=-1)
            cv_results[metric] = scores
            print(f"   {metric:20s}: {np.mean(scores):.4f} Â± {np.std(scores):.4f}")
        
        self.cv_scores = cv_results
        return cv_results
    
    def hyperparameter_tuning(self, X_train, y_train, n_iter: int = 50) -> Dict:
        """
        Hyperparameter tuning ile en iyi parametreleri bul.
        
        Args:
            X_train: EÄŸitim Ã¶zellikleri
            y_train: EÄŸitim etiketleri
            n_iter: RandomizedSearchCV iterasyon sayÄ±sÄ±
            
        Returns:
            En iyi parametreler
        """
        print(f"\nğŸ”§ Hyperparameter Tuning baÅŸlÄ±yor ({n_iter} iterasyon)...")
        print(f"   Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...\n")
        
        # Model tipine gÃ¶re parametre grid'i belirle
        if self.model_tipi == "xgboost":
            import xgboost as xgb
            param_distributions = {
                'n_estimators': [100, 200, 300, 500],
                'max_depth': [3, 5, 7, 9, 11],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'subsample': [0.6, 0.8, 1.0],
                'colsample_bytree': [0.6, 0.8, 1.0],
                'gamma': [0, 0.1, 0.2, 0.5],
                'min_child_weight': [1, 3, 5, 7],
            }
            base_model = xgb.XGBClassifier(random_state=RASTGELE_TOHUM)
            
        elif self.model_tipi == "lightgbm":
            import lightgbm as lgb
            param_distributions = {
                'n_estimators': [100, 200, 300, 500],
                'num_leaves': [15, 31, 63, 127],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'subsample': [0.6, 0.8, 1.0],
                'colsample_bytree': [0.6, 0.8, 1.0],
                'min_child_samples': [10, 20, 30, 50],
            }
            base_model = lgb.LGBMClassifier(random_state=RASTGELE_TOHUM)
            
        elif self.model_tipi == "svm":
            from sklearn.svm import LinearSVC
            param_distributions = {
                'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0],
                'loss': ['hinge', 'squared_hinge'],
                'max_iter': [1000, 2000, 5000],
            }
            base_model = LinearSVC(random_state=RASTGELE_TOHUM)
        
        # RandomizedSearchCV ile arama yap
        random_search = RandomizedSearchCV(
            estimator=base_model,
            param_distributions=param_distributions,
            n_iter=n_iter,
            cv=5,
            scoring='f1_macro',
            n_jobs=-1,
            random_state=RASTGELE_TOHUM,
            verbose=1
        )
        
        random_search.fit(X_train, y_train)
        
        print(f"\nâœ“ Hyperparameter tuning tamamlandÄ±!")
        print(f"\nğŸ† En iyi parametreler:")
        for param, value in random_search.best_params_.items():
            print(f"   {param}: {value}")
        print(f"\nğŸ“ˆ En iyi CV skoru: {random_search.best_score_:.4f}")
        
        # En iyi modeli kullan
        self.model = random_search.best_estimator_
        
        return random_search.best_params_
    
    def confusion_matrix_ciz(self, y_true, y_pred, dosya_adi: str = "confusion_matrix.png"):
        """KarÄ±ÅŸÄ±klÄ±k matrisi Ã§iz ve kaydet."""
        cm = confusion_matrix(y_true, y_pred)
        
        fig, ax = plt.subplots(figsize=GORSEL_AYARLARI['confusion_matrix_figsize'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar_kws={'label': 'SayÄ±'})
        ax.set_xlabel('Tahmin Edilen SÄ±nÄ±f', fontsize=12)
        ax.set_ylabel('GerÃ§ek SÄ±nÄ±f', fontsize=12)
        ax.set_title(f'KarÄ±ÅŸÄ±klÄ±k Matrisi - {self.model_tipi.upper()}', fontsize=14, fontweight='bold')
        
        kayit_yolu = GORSELLER_KLASORU / dosya_adi
        fig.savefig(kayit_yolu, dpi=GORSEL_AYARLARI['dpi'], bbox_inches='tight')
        plt.close()
        print(f"   âœ“ KarÄ±ÅŸÄ±klÄ±k matrisi kaydedildi: {kayit_yolu}")
    
    def ozellik_onemi_ciz(self, top_n: int = 20):
        """Ã–zellik Ã¶nemini Ã§iz (gradient boosting iÃ§in)."""
        if self.model_tipi not in ["xgboost", "lightgbm"]:
            print(f"   âš ï¸  Ã–zellik Ã¶nemi sadece gradient boosting modellerde desteklenir")
            return
        
        if not hasattr(self.model, 'feature_importances_'):
            print(f"   âš ï¸  Model Ã¶zellik Ã¶nemini desteklemiyor")
            return
        
        importances = self.model.feature_importances_
        indices = np.argsort(importances)[::-1][:top_n]
        
        fig, ax = plt.subplots(figsize=GORSEL_AYARLARI['feature_importance_figsize'])
        ax.barh(range(top_n), importances[indices], color='steelblue')
        ax.set_yticks(range(top_n))
        ax.set_yticklabels([self.feature_names[i] for i in indices])
        ax.set_xlabel('Ã–nem Skoru', fontsize=12)
        ax.set_title(f'En Ã–nemli {top_n} Ã–zellik - {self.model_tipi.upper()}', fontsize=14, fontweight='bold')
        ax.invert_yaxis()
        plt.tight_layout()
        
        kayit_yolu = GORSELLER_KLASORU / f"ozellik_onemi_{self.model_tipi}.png"
        fig.savefig(kayit_yolu, dpi=GORSEL_AYARLARI['dpi'], bbox_inches='tight')
        plt.close()
        print(f"   âœ“ Ã–zellik Ã¶nemi grafiÄŸi kaydedildi: {kayit_yolu}")
    
    def roc_curve_ciz(self, X_test, y_test, dosya_adi: str = "roc_curves.png"):
        """ROC eÄŸrilerini Ã§iz (multi-class iÃ§in one-vs-rest)."""
        if not hasattr(self.model, 'predict_proba'):
            print(f"   âš ï¸  ROC eÄŸrisi sadece olasÄ±lÄ±k tahminini destekleyen modeller iÃ§in Ã§izilebilir")
            return
        
        try:
            from sklearn.preprocessing import label_binarize
            from sklearn.metrics import roc_curve, auc
            
            # Tahminler
            y_prob = self.model.predict_proba(X_test)
            n_classes = y_prob.shape[1]
            
            # One-hot encoding
            y_test_bin = label_binarize(y_test, classes=range(n_classes))
            
            # Her sÄ±nÄ±f iÃ§in ROC hesapla
            fpr = dict()
            tpr = dict()
            roc_auc = dict()
            
            for i in range(n_classes):
                fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
                roc_auc[i] = auc(fpr[i], tpr[i])
            
            # Grafik Ã§iz
            fig, ax = plt.subplots(figsize=GORSEL_AYARLARI['roc_curve_figsize'])
            
            colors = ['blue', 'red', 'green', 'orange']
            class_names = ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']
            
            for i, color in zip(range(n_classes), colors):
                ax.plot(fpr[i], tpr[i], color=color, lw=2,
                       label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')
            
            ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Rastgele (AUC = 0.5)')
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate', fontsize=12)
            ax.set_ylabel('True Positive Rate', fontsize=12)
            ax.set_title(f'ROC EÄŸrileri - {self.model_tipi.upper()}', fontsize=14, fontweight='bold')
            ax.legend(loc="lower right")
            ax.grid(alpha=0.3)
            plt.tight_layout()
            
            kayit_yolu = GORSELLER_KLASORU / dosya_adi
            fig.savefig(kayit_yolu, dpi=GORSEL_AYARLARI['dpi'], bbox_inches='tight')
            plt.close()
            print(f"   âœ“ ROC eÄŸrileri kaydedildi: {kayit_yolu}")
            
        except Exception as e:
            print(f"   âš ï¸  ROC eÄŸrisi Ã§izilemedi: {e}")
    
    def precision_recall_curve_ciz(self, X_test, y_test, dosya_adi: str = "precision_recall_curves.png"):
        """Precision-Recall eÄŸrilerini Ã§iz (multi-class iÃ§in one-vs-rest)."""
        if not hasattr(self.model, 'predict_proba'):
            print(f"   âš ï¸  Precision-Recall eÄŸrisi sadece olasÄ±lÄ±k tahminini destekleyen modeller iÃ§in Ã§izilebilir")
            return
        
        try:
            from sklearn.preprocessing import label_binarize
            from sklearn.metrics import precision_recall_curve, average_precision_score
            
            # Tahminler
            y_prob = self.model.predict_proba(X_test)
            n_classes = y_prob.shape[1]
            
            # One-hot encoding
            y_test_bin = label_binarize(y_test, classes=range(n_classes))
            
            # Her sÄ±nÄ±f iÃ§in precision-recall hesapla
            precision = dict()
            recall = dict()
            avg_precision = dict()
            
            for i in range(n_classes):
                precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_prob[:, i])
                avg_precision[i] = average_precision_score(y_test_bin[:, i], y_prob[:, i])
            
            # Grafik Ã§iz
            fig, ax = plt.subplots(figsize=GORSEL_AYARLARI['roc_curve_figsize'])
            
            colors = ['blue', 'red', 'green', 'orange']
            class_names = ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']
            
            for i, color in zip(range(n_classes), colors):
                ax.plot(recall[i], precision[i], color=color, lw=2,
                       label=f'{class_names[i]} (AP = {avg_precision[i]:.3f})')
            
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('Recall', fontsize=12)
            ax.set_ylabel('Precision', fontsize=12)
            ax.set_title(f'Precision-Recall EÄŸrileri - {self.model_tipi.upper()}', fontsize=14, fontweight='bold')
            ax.legend(loc="lower left")
            ax.grid(alpha=0.3)
            plt.tight_layout()
            
            kayit_yolu = GORSELLER_KLASORU / dosya_adi
            fig.savefig(kayit_yolu, dpi=GORSEL_AYARLARI['dpi'], bbox_inches='tight')
            plt.close()
            print(f"   âœ“ Precision-Recall eÄŸrileri kaydedildi: {kayit_yolu}")
            
        except Exception as e:
            print(f"   âš ï¸  Precision-Recall eÄŸrisi Ã§izilemedi: {e}")
    
    def grafik_ciz(self, X_test, y_test):
        """TÃ¼m grafikleri Ã§iz."""
        print(f"\nğŸ“Š Grafikler oluÅŸturuluyor...")
        
        y_test_pred = self.tahmin_yap(X_test)
        
        # 1. Confusion Matrix
        self.confusion_matrix_ciz(y_test, y_test_pred)
        
        # 2. Feature Importance (gradient boosting iÃ§in)
        self.ozellik_onemi_ciz()
        
        # 3. ROC Curves
        self.roc_curve_ciz(X_test, y_test)
        
        # 4. Precision-Recall Curves
        self.precision_recall_curve_ciz(X_test, y_test)
    
    def rapor_olustur(self):
        """DetaylÄ± performans raporu oluÅŸtur."""
        rapor_yolu = RAPORLAR_KLASORU / f"rapor_{self.model_tipi}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(rapor_yolu, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write(f"MRI SINIFLANDIRMA MODEL RAPORU\n")
            f.write("="*70 + "\n\n")
            
            f.write(f"Model Tipi: {self.model_tipi.upper()}\n")
            f.write(f"Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"SMOTE Aktif: {self.smote_aktif}\n")
            f.write(f"Feature Selection Aktif: {self.feature_selection_aktif}\n")
            f.write("\n" + "="*70 + "\n")
            f.write("PERFORMANS METRÄ°KLERÄ°\n")
            f.write("="*70 + "\n\n")
            
            if self.metrikler:
                for key, value in self.metrikler.items():
                    if key not in ['confusion_matrix', 'classification_report']:
                        if isinstance(value, float):
                            f.write(f"{key}: {value:.4f}\n")
                        else:
                            f.write(f"{key}: {value}\n")
                
                if 'classification_report' in self.metrikler:
                    f.write("\n" + "-"*70 + "\n")
                    f.write("DETAYLI SINIF RAPORU\n")
                    f.write("-"*70 + "\n\n")
                    f.write(self.metrikler['classification_report'])
            
            # Cross-validation sonuÃ§larÄ±
            if self.cv_scores:
                f.write("\n" + "="*70 + "\n")
                f.write("CROSS-VALIDATION SONUÃ‡LARI\n")
                f.write("="*70 + "\n\n")
                for metric, scores in self.cv_scores.items():
                    f.write(f"{metric}: {np.mean(scores):.4f} Â± {np.std(scores):.4f}\n")
        
        print(f"   âœ“ Rapor kaydedildi: {rapor_yolu}")
    
    def model_kaydet(self, dosya_adi: Optional[str] = None):
        """Modeli ve metadata'sÄ±nÄ± kaydet."""
        if dosya_adi is None:
            zaman_damgasi = datetime.now().strftime("%Y%m%d_%H%M%S")
            dosya_adi = f"{self.model_tipi}_{zaman_damgasi}.pkl"
        
        kayit_yolu = MODELS_KLASORU / dosya_adi
        
        # Modeli kaydet
        with open(kayit_yolu, 'wb') as f:
            pickle.dump(self.model, f)
        
        print(f"\nğŸ’¾ Model kaydedildi: {kayit_yolu}")
        
        # Metadata kaydet
        metadata = {
            'model_tipi': self.model_tipi,
            'tarih': datetime.now().isoformat(),
            'metrikler': {k: float(v) if isinstance(v, (np.floating, float)) else v 
                         for k, v in self.metrikler.items() 
                         if not isinstance(v, (np.ndarray, str))},
            'feature_names': self.feature_names,
            'ayarlar': GB_AYARLARI if self.model_tipi in ["xgboost", "lightgbm"] else SVM_AYARLARI
        }
        
        metadata_yolu = kayit_yolu.with_suffix('.json')
        with open(metadata_yolu, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"   âœ“ Metadata kaydedildi: {metadata_yolu}")
    
    def tam_egitim_yap(self, hyperparameter_tuning_aktif: bool = False):
        """
        Tam eÄŸitim pipeline'Ä± Ã§alÄ±ÅŸtÄ±r.
        
        Args:
            hyperparameter_tuning_aktif: Hyperparameter tuning yapÄ±lsÄ±n mÄ±?
        """
        # 1. Veri yÃ¼kle ve hazÄ±rla
        X_train, X_val, X_test, y_train, y_val, y_test = self.veri_yukle()
        
        # 2. Feature selection (opsiyonel)
        if self.feature_selection_aktif:
            X_train = self.feature_selection(X_train, y_train, k=15)
            # Validation ve test setlerine de uygula
            if self.selected_features:
                X_val = X_val[self.selected_features]
                X_test = X_test[self.selected_features]
        
        # 3. Cross-validation (model eÄŸitiminden Ã¶nce)
        print(f"\n{'='*60}")
        print(f"ğŸ“Š CROSS-VALIDATION")
        print(f"{'='*60}")
        self.cross_validate(X_train, y_train, cv_folds=5)
        
        # 4. Hyperparameter tuning (opsiyonel)
        if hyperparameter_tuning_aktif:
            print(f"\n{'='*60}")
            print(f"ğŸ”§ HYPERPARAMETER TUNING")
            print(f"{'='*60}")
            best_params = self.hyperparameter_tuning(X_train, y_train, n_iter=30)
        else:
            # Normal model oluÅŸtur
            self.model_olustur()
        
        # 5. Model eÄŸit
        if not hyperparameter_tuning_aktif:
            # Hyperparameter tuning zaten eÄŸitiyor, tekrar eÄŸitmeye gerek yok
            self.egit(X_train, y_train, X_val, y_val)
        
        # 6. DeÄŸerlendir
        print(f"\n{'='*60}")
        print(f"ğŸ“ˆ DEÄERLENDÄ°RME")
        print(f"{'='*60}")
        
        # DoÄŸrulama seti
        val_metrikler = self.degerlendir(X_val, y_val, "DoÄŸrulama")
        
        # Test seti
        test_metrikler = self.degerlendir(X_test, y_test, "Test")
        self.metrikler = test_metrikler
        
        # 4. GÃ¶rselleÅŸtir
        print(f"\n{'='*60}")
        print(f"ğŸ“Š GÃ–RSELLEÅTÄ°RME")
        print(f"{'='*60}")
        
        self.grafik_ciz(X_test, y_test)
        
        # 5. Rapor oluÅŸtur
        print(f"\n{'='*60}")
        print(f"ğŸ“„ RAPOR OLUÅTURMA")
        print(f"{'='*60}")
        
        self.rapor_olustur()
        
        # 6. Modeli kaydet
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ MODEL KAYDETME")
        print(f"{'='*60}")
        self.model_kaydet()
        
        print(f"\n{'='*60}")
        print(f"âœ… EÄÄ°TÄ°M TAMAMLANDI!")
        print(f"{'='*60}")
        print(f"\nğŸ“ Ã‡Ä±ktÄ±lar: {CIKTI_KLASORU}")


def main():
    """
    Ana program - Model seÃ§imi ve eÄŸitim.
    
    Bu fonksiyon kullanÄ±cÄ±ya hangi modeli eÄŸitmek istediÄŸini sorar
    ve seÃ§ilen model(ler)i eÄŸitir.
    """
    print(f"\n{'='*70}")
    print(f"ğŸ§  MRI SINIFLANDIRMA - MODEL EÄÄ°TÄ°MÄ°")
    print(f"{'='*70}")
    
    print(f"\nğŸ“‹ Mevcut Modeller:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"  1ï¸âƒ£  XGBoost (Gradient Boosting)")
    print(f"      â””â”€ YÃ¼ksek doÄŸruluk, gÃ¼Ã§lÃ¼ performans")
    print(f"      â””â”€ Ã–nerilen model â­")
    print(f"")
    print(f"  2ï¸âƒ£  LightGBM (Gradient Boosting)")
    print(f"      â””â”€ HÄ±zlÄ± eÄŸitim, bÃ¼yÃ¼k veri setleri iÃ§in")
    print(f"      â””â”€ XGBoost'a alternatif")
    print(f"")
    print(f"  3ï¸âƒ£  Linear SVM")
    print(f"      â””â”€ Basit ve hÄ±zlÄ±")
    print(f"      â””â”€ Test ve karÅŸÄ±laÅŸtÄ±rma iÃ§in")
    print(f"")
    print(f"  4ï¸âƒ£  TÃ¼mÃ¼ (SÄ±rayla hepsini eÄŸit)")
    print(f"      â””â”€ KarÅŸÄ±laÅŸtÄ±rmalÄ± analiz")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    secim = input(f"\nğŸ¯ Model seÃ§iminiz (1-4): ").strip()
    
    # GeliÅŸmiÅŸ Ã¶zellikler
    print(f"\nâš™ï¸  GeliÅŸmiÅŸ Ã–zellikler:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    smote_input = input(f"SMOTE ile veri dengeleme? (E/h) [E]: ").strip().lower()
    smote_aktif = smote_input != 'h'
    
    tuning_input = input(f"Hyperparameter tuning? (e/H) [H]: ").strip().lower()
    hyperparameter_tuning_aktif = tuning_input == 'e'
    
    feature_sel_input = input(f"Feature selection? (e/H) [H]: ").strip().lower()
    feature_selection_aktif = feature_sel_input == 'e'
    
    # Model seÃ§im haritasÄ±
    model_map = {
        '1': ['xgboost'],
        '2': ['lightgbm'],
        '3': ['svm'],
        '4': ['xgboost', 'lightgbm', 'svm']
    }
    
    if secim not in model_map:
        print(f"\nâŒ GeÃ§ersiz seÃ§im! LÃ¼tfen 1-4 arasÄ± bir sayÄ± girin.")
        return
    
    modeller = model_map[secim]
    
    # SeÃ§ilen modelleri eÄŸit
    for i, model_tipi in enumerate(modeller, 1):
        if len(modeller) > 1:
            print(f"\n\n{'#'*70}")
            print(f"# [{i}/{len(modeller)}] {model_tipi.upper()} MODELÄ° EÄÄ°TÄ°LÄ°YOR")
            print(f"{'#'*70}\n")
        
        try:
            # Model eÄŸiticiyi baÅŸlat
            egitici = ModelEgitici(
                model_tipi=model_tipi,
                smote_aktif=smote_aktif,
                feature_selection_aktif=feature_selection_aktif
            )
            
            # Tam eÄŸitim yap
            egitici.tam_egitim_yap(hyperparameter_tuning_aktif=hyperparameter_tuning_aktif)
            
        except FileNotFoundError as e:
            print(f"\nâŒ Hata: {e}")
            print(f"\nğŸ’¡ Ã‡Ã¶zÃ¼m: Ã–nce gÃ¶rÃ¼ntÃ¼ iÅŸleme adÄ±mlarÄ±nÄ± tamamlayÄ±n:")
            print(f"   cd ../goruntu_isleme")
            print(f"   python ana_islem.py")
            break
        except Exception as e:
            print(f"\nâŒ {model_tipi} eÄŸitimi baÅŸarÄ±sÄ±z: {e}")
            import traceback
            traceback.print_exc()
    
    if len(modeller) > 1:
        print(f"\n\n{'='*70}")
        print(f"âœ… TÃœM EÄÄ°TÄ°MLER TAMAMLANDI!")
        print(f"{'='*70}")
        print(f"\nğŸ“Š SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±rmak iÃ§in:")
        print(f"   {GORSELLER_KLASORU}")


if __name__ == "__main__":
    main()
