"""
ozellik_cikarici.py
-------------------
Ä°ÅŸlenmiÅŸ gÃ¶rÃ¼ntÃ¼lerden Ã¶zellik Ã§Ä±karma ve CSV oluÅŸturma modÃ¼lÃ¼.
"""

import os
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional
from PIL import Image
from tqdm import tqdm
from scipy import ndimage
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, MaxAbsScaler
from multiprocessing import Pool, cpu_count
from functools import partial

from ayarlar import *


def _ozellik_cikar_wrapper(goruntu_yolu: str, sinif_adi: str) -> Optional[Dict]:
    """âš¡ Paralel Ã¶zellik Ã§Ä±karma iÃ§in wrapper fonksiyon."""
    try:
        cikarici = OzellikCikarici()
        ozellikler = cikarici.tek_goruntu_ozellikleri(str(goruntu_yolu))
        
        if ozellikler:
            ozellikler["sinif"] = sinif_adi
            ozellikler["etiket"] = SINIF_ETIKETI[sinif_adi]
            ozellikler["tam_yol"] = str(goruntu_yolu)
            return ozellikler
    except Exception:
        pass
    return None


class OzellikCikarici:
    """GÃ¶rÃ¼ntÃ¼lerden Ã¶zellik Ã§Ä±karma ve CSV oluÅŸturma sÄ±nÄ±fÄ±."""
    
    def __init__(self):
        """Ã–zellik Ã§Ä±karÄ±cÄ±yÄ± baÅŸlat."""
        self.n_jobs = max(1, cpu_count() - 1)  # Bir Ã§ekirdek sisteme bÄ±rak
    
    def tek_goruntu_ozellikleri(self, goruntu_yolu: str) -> Optional[Dict]:
        """
        Tek bir gÃ¶rÃ¼ntÃ¼den Ã¶zellikler Ã§Ä±kar.
        
        Bu fonksiyon, bir MRI gÃ¶rÃ¼ntÃ¼sÃ¼nden makine Ã¶ÄŸrenmesi iÃ§in kullanÄ±lacak
        sayÄ±sal Ã¶zellikleri hesaplar. Bu Ã¶zellikler gÃ¶rÃ¼ntÃ¼nÃ¼n iÃ§eriÄŸini temsil eder.
        
        Ã‡Ä±karÄ±lan Ã¶zellikler:
        - Dosya bilgileri: ad, boyut (byte)
        - GÃ¶rÃ¼ntÃ¼ boyutlarÄ±: geniÅŸlik, yÃ¼kseklik, en-boy oranÄ±, piksel sayÄ±sÄ±
        - YoÄŸunluk istatistikleri: ortalama, std sapma, min, max, medyan, percentile'ler
        - Doku Ã¶zellikleri: entropi (bilgi miktarÄ±), kontrast, homojenlik, enerji
        
        Args:
            goruntu_yolu: GÃ¶rÃ¼ntÃ¼ dosyasÄ±nÄ±n tam yolu
            
        Returns:
            Ã–zellikler sÃ¶zlÃ¼ÄŸÃ¼ veya hata durumunda None
        """
        try:
            # 1. DOSYA BÄ°LGÄ°LERÄ°NÄ° AL
            dosya_adi = os.path.basename(goruntu_yolu)  # Sadece dosya adÄ±
            boyut_bayt = os.path.getsize(goruntu_yolu)  # Dosya boyutu (byte)
            
            # 2. GÃ–RÃœNTÃœYÃœ YÃœKLE VE GRÄ° TONLAMAYA Ã‡EVÄ°R
            goruntu = Image.open(goruntu_yolu)
            if goruntu.mode != 'L':  # EÄŸer renkli ise
                goruntu = goruntu.convert('L')  # Gri tonlamaya Ã§evir
            
            # 3. BOYUT BÄ°LGÄ°LERÄ°NÄ° HESAPLA
            genislik, yukseklik = goruntu.size
            en_boy_orani = genislik / yukseklik if yukseklik > 0 else 0.0
            
            # 4. PÄ°KSEL VERÄ°LERÄ°NÄ° NUMPY ARRAY'E Ã‡EVÄ°R
            piksel_array = np.array(goruntu, dtype=np.float32)
            
            # 5. YOÄžUNLUK Ä°STATÄ°STÄ°KLERÄ°NÄ° HESAPLA
            # Temel istatistikler
            ort_yogunluk = float(np.mean(piksel_array))     # Ortalama piksel deÄŸeri
            std_yogunluk = float(np.std(piksel_array))      # Standart sapma (daÄŸÄ±lÄ±m)
            min_yogunluk = float(np.min(piksel_array))      # En dÃ¼ÅŸÃ¼k deÄŸer
            max_yogunluk = float(np.max(piksel_array))      # En yÃ¼ksek deÄŸer
            
            # YÃ¼zdelik deÄŸerleri (percentiles) - daÄŸÄ±lÄ±mÄ± daha iyi anlamak iÃ§in
            p1_yogunluk = float(np.percentile(piksel_array, 1))    # %1'lik dilim
            p25_yogunluk = float(np.percentile(piksel_array, 25))  # 1. Ã§eyrek
            p50_yogunluk = float(np.percentile(piksel_array, 50))  # Medyan (ortanca)
            p75_yogunluk = float(np.percentile(piksel_array, 75))  # 3. Ã§eyrek
            p99_yogunluk = float(np.percentile(piksel_array, 99))  # %99'luk dilim
            
            # 6. SHANNON ENTROPÄ°SÄ°NÄ° HESAPLA
            # Entropi, gÃ¶rÃ¼ntÃ¼deki bilgi miktarÄ±nÄ± / karmaÅŸÄ±klÄ±ÄŸÄ± Ã¶lÃ§er
            # YÃ¼ksek entropi = fazla detay, dÃ¼ÅŸÃ¼k entropi = dÃ¼z/homojen gÃ¶rÃ¼ntÃ¼
            hist, _ = np.histogram(piksel_array.astype(np.uint8), bins=256, range=(0, 256))
            hist = hist / hist.sum()  # HistogramÄ± normalize et (olasÄ±lÄ±klara Ã§evir)
            hist = hist[hist > 0]     # SÄ±fÄ±r olmayan deÄŸerleri al
            entropi = float(-np.sum(hist * np.log2(hist)))  # Shannon entropi formÃ¼lÃ¼
            
            # 7. KONTRAST HESAPLA (Laplacian varyansÄ±)
            # Kontrast, gÃ¶rÃ¼ntÃ¼deki keskinliÄŸi / kenar yoÄŸunluÄŸunu Ã¶lÃ§er
            laplacian = ndimage.laplace(piksel_array)  # Laplacian filtresi uygula
            kontrast = float(np.var(laplacian))        # VaryansÄ± al
            
            # 8. EK DOKU Ã–ZELLÄ°KLERÄ°NÄ° HESAPLA
            # Homojenlik: GÃ¶rÃ¼ntÃ¼ ne kadar dÃ¼zgÃ¼n? (dÃ¼ÅŸÃ¼k std = yÃ¼ksek homojenlik)
            homojenlik = 1.0 / (1.0 + std_yogunluk)
            
            # Enerji: HistogramÄ±n ikinci momenti (Ã¼niformluÄŸu Ã¶lÃ§er)
            enerji = float(np.sum(hist ** 2))
            
            # 9. GELÄ°ÅžMÄ°Åž Ä°STATÄ°STÄ°KSEL Ã–ZELLÄ°KLER
            # Skewness (Ã‡arpÄ±klÄ±k): DaÄŸÄ±lÄ±mÄ±n simetrisini Ã¶lÃ§er
            # Pozitif = saÄŸa Ã§arpÄ±k, negatif = sola Ã§arpÄ±k, 0 = simetrik
            from scipy.stats import skew, kurtosis
            carpiklik = float(skew(piksel_array.flatten()))
            
            # Kurtosis (BasÄ±klÄ±k): DaÄŸÄ±lÄ±mÄ±n kuyruk kalÄ±nlÄ±ÄŸÄ±nÄ± Ã¶lÃ§er
            # YÃ¼ksek = sivri tepe ve kalÄ±n kuyruklar, dÃ¼ÅŸÃ¼k = dÃ¼z daÄŸÄ±lÄ±m
            basiklik = float(kurtosis(piksel_array.flatten()))
            
            # 10. GRADYAN Ã–ZELLÄ°KLERÄ° (Kenar YoÄŸunluÄŸu)
            # Gradyan, gÃ¶rÃ¼ntÃ¼deki deÄŸiÅŸim hÄ±zÄ±nÄ± Ã¶lÃ§er (kenarlarÄ± yakalar)
            grad_y, grad_x = np.gradient(piksel_array.astype(np.float32))
            gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
            ortalama_gradyan = float(np.mean(gradient_magnitude))
            max_gradyan = float(np.max(gradient_magnitude))
            
            # 11. OTSU EÅžÄ°ÄžÄ° ANALÄ°ZÄ°
            # Otsu yÃ¶ntemi optimal eÅŸik deÄŸerini otomatik bulur
            # Bu deÄŸer, beyin-arka plan ayrÄ±mÄ± iÃ§in ipucu verir
            try:
                from skimage.filters import threshold_otsu
                otsu_esik = float(threshold_otsu(goruntu))
            except ImportError:
                # scikit-image yoksa basit hesaplama
                otsu_esik = float(np.mean(piksel_array))
            
            # 12. TÃœM Ã–ZELLÄ°KLERÄ° SÃ–ZLÃœKTE TOPLA VE DÃ–NDÃœR
            return {
                "dosya_adi": dosya_adi,
                "boyut_bayt": int(boyut_bayt),
                "genislik": int(genislik),
                "yukseklik": int(yukseklik),
                "en_boy_orani": round(en_boy_orani, 4),
                "piksel_sayisi": int(genislik * yukseklik),
                "ort_yogunluk": round(ort_yogunluk, 2),
                "std_yogunluk": round(std_yogunluk, 2),
                "min_yogunluk": round(min_yogunluk, 2),
                "max_yogunluk": round(max_yogunluk, 2),
                "p1_yogunluk": round(p1_yogunluk, 2),
                "p25_yogunluk": round(p25_yogunluk, 2),
                "medyan_yogunluk": round(p50_yogunluk, 2),
                "p75_yogunluk": round(p75_yogunluk, 2),
                "p99_yogunluk": round(p99_yogunluk, 2),
                "entropi": round(entropi, 4),
                "kontrast": round(kontrast, 2),
                "homojenlik": round(homojenlik, 4),
                "enerji": round(enerji, 4),
                # Yeni geliÅŸmiÅŸ Ã¶zellikler
                "carpiklik": round(carpiklik, 4),
                "basiklik": round(basiklik, 4),
                "ortalama_gradyan": round(ortalama_gradyan, 2),
                "max_gradyan": round(max_gradyan, 2),
                "otsu_esik": round(otsu_esik, 2),
            }
            
        except Exception as e:
            print(f"[HATA] Ã–zellik Ã§Ä±karÄ±lamadÄ± {goruntu_yolu}: {e}")
            return None
    
    def csv_olustur(self, giris_klasoru: Path = CIKTI_KLASORU, 
                    cikti_csv: Optional[Path] = None) -> pd.DataFrame:
        """
        KlasÃ¶rdeki tÃ¼m gÃ¶rÃ¼ntÃ¼lerden Ã¶zellik Ã§Ä±kar ve CSV oluÅŸtur.
        
        Bu fonksiyon, makine Ã¶ÄŸrenmesi iÃ§in kullanÄ±lacak veri setini hazÄ±rlar.
        Ä°ÅŸlenmiÅŸ gÃ¶rÃ¼ntÃ¼ klasÃ¶rÃ¼nÃ¼ tarar, her gÃ¶rÃ¼ntÃ¼ iÃ§in 20+ Ã¶zellik hesaplar
        ve sonuÃ§larÄ± tek bir CSV dosyasÄ±na birleÅŸtirir.
        
        Ã‡alÄ±ÅŸma akÄ±ÅŸÄ±:
        1. TÃ¼m sÄ±nÄ±f klasÃ¶rlerini tara (NonDemented, MildDemented, vb.)
        2. Her gÃ¶rÃ¼ntÃ¼ iÃ§in tek_goruntu_ozellikleri() Ã§aÄŸÄ±r
        3. SÄ±nÄ±f adÄ± ve etiketi ekle
        4. TÃ¼m sonuÃ§larÄ± DataFrame'de birleÅŸtir
        5. CSV'ye kaydet
        
        CSV formatÄ±:
        | dosya_adi | sinif | etiket | genislik | yukseklik | ... | entropi |
        |-----------|-------|--------|----------|-----------|-----|----------|
        | img1.jpg  | NonDemented | 0 | 256 | 256 | ... | 5.23 |
        
        Bu CSV, model/train.py tarafÄ±ndan okunur ve eÄŸitimde kullanÄ±lÄ±r.
        
        Args:
            giris_klasoru: Ä°ÅŸlenmiÅŸ gÃ¶rÃ¼ntÃ¼lerin bulunduÄŸu klasÃ¶r
            cikti_csv: CSV dosyasÄ±nÄ±n kaydedileceÄŸi yol (None ise varsayÄ±lan)
            
        Returns:
            Pandas DataFrame (tÃ¼m Ã¶zellikler ve etiketler)
        """
        # VarsayÄ±lan CSV yolunu belirle
        if cikti_csv is None:
            cikti_csv = CIKTI_KLASORU / CSV_DOSYA_ADI
        
        tum_ozellikler = []  # TÃ¼m gÃ¶rÃ¼ntÃ¼lerin Ã¶zelliklerini saklayacak liste
        
        print(f"\nâš¡ Ã–zellikler Ã§Ä±karÄ±lÄ±yor (paralel: {self.n_jobs} Ã§ekirdek)...\n")
        
        # Her sÄ±nÄ±f iÃ§in dÃ¶ngÃ¼
        for sinif_adi in SINIF_KLASORLERI:
            sinif_klasoru = giris_klasoru / sinif_adi
            
            # KlasÃ¶r yoksa uyar ve devam et
            if not sinif_klasoru.exists():
                print(f"[UYARI] KlasÃ¶r bulunamadÄ±: {sinif_klasoru}")
                continue
            
            # KlasÃ¶rdeki tÃ¼m gÃ¶rÃ¼ntÃ¼leri bul (.png ve .jpg)
            gorseller = list(sinif_klasoru.glob("*.png")) + list(sinif_klasoru.glob("*.jpg"))
            
            # âš¡ Paralel Ã¶zellik Ã§Ä±karma
            with Pool(processes=self.n_jobs) as pool:
                partial_func = partial(_ozellik_cikar_wrapper, sinif_adi=sinif_adi)
                sonuclar = list(tqdm(
                    pool.imap(partial_func, gorseller),
                    total=len(gorseller),
                    desc=f"{sinif_adi} iÅŸleniyor (paralel)"
                ))
            
            # None olmayan sonuÃ§larÄ± ekle
            tum_ozellikler.extend([s for s in sonuclar if s is not None])
        
        if not tum_ozellikler:
            print("[HATA] HiÃ§ Ã¶zellik Ã§Ä±karÄ±lamadÄ±!")
            return pd.DataFrame()
        
        # DataFrame oluÅŸtur
        df = pd.DataFrame(tum_ozellikler)
        
        # CSV'ye kaydet
        df.to_csv(cikti_csv, index=False, encoding='utf-8')
        print(f"\nâœ“ CSV kaydedildi: {cikti_csv}")
        print(f"  Toplam {len(df)} gÃ¶rÃ¼ntÃ¼")
        print(f"\nSÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
        print(df['sinif'].value_counts().to_string())
        
        return df
    
    def scaling_uygula(self, giris_csv: Optional[Path] = None,
                      cikti_csv: Optional[Path] = None,
                      metod: str = SCALING_METODU) -> pd.DataFrame:
        """
        CSV dosyasÄ±ndaki Ã¶zelliklere Ã¶lÃ§eklendirme (scaling) uygula.
        
        Neden Ã¶lÃ§eklendirme gerekli?
        - Makine Ã¶ÄŸrenmesi modelleri, farklÄ± Ã¶lÃ§eklerdeki Ã¶zelliklerle iyi Ã§alÄ±ÅŸamaz
        - Ã–rn: genislik=256, entropi=5.2 -> model genislik'e aÅŸÄ±rÄ± aÄŸÄ±rlÄ±k verir
        - TÃ¼m Ã¶zellikleri aynÄ± Ã¶lÃ§eÄŸe getirerek adil bir Ã¶ÄŸrenme saÄŸlanÄ±r
        
        Desteklenen Ã¶lÃ§eklendirme metodlarÄ±:
        
        1. minmax: Min-Max normalizasyonu
           - TÃ¼m deÄŸerleri [0, 1] aralÄ±ÄŸÄ±na sÄ±kÄ±ÅŸtÄ±rÄ±r
           - FormÃ¼l: (x - min) / (max - min)
           - Avantaj: Basit, hÄ±zlÄ±
           - Dezavantaj: AykÄ±rÄ± deÄŸerlere duyarlÄ±
        
        2. robust: Robust Scaler (Ã–nerilen â­)
           - Medyan ve IQR (interquartile range) kullanÄ±r
           - FormÃ¼l: (x - median) / IQR
           - Avantaj: AykÄ±rÄ± deÄŸerlere karÅŸÄ± dayanÄ±klÄ±
           - MRI gÃ¶rÃ¼ntÃ¼lerinde gÃ¼rÃ¼ltÃ¼ olabilir, bu yÃ¼zden robust tercih edilir
        
        3. standard: Standart (Z-score) normalizasyonu
           - Ortalama ve standart sapma kullanÄ±r
           - FormÃ¼l: (x - mean) / std
           - Avantaj: Normal daÄŸÄ±lÄ±m saÄŸlar (mean=0, std=1)
           - KullanÄ±m: SVM ve lineer modeller iÃ§in uygun
        
        4. maxabs: Max Absolute Scaler
           - Maksimum mutlak deÄŸere bÃ¶ler
           - FormÃ¼l: x / max(|x|)
           - DeÄŸerler [-1, 1] aralÄ±ÄŸÄ±nda olur
        
        Args:
            giris_csv: Okunacak CSV dosyasÄ± (None ise varsayÄ±lan)
            cikti_csv: Kaydedilecek CSV dosyasÄ± (None ise orijinal Ã¼zerine yazar)
            metod: Ã–lÃ§eklendirme metodu ('minmax', 'robust', 'standard', 'maxabs')
            
        Returns:
            Ã–lÃ§eklendirilmiÅŸ DataFrame
        """
        if giris_csv is None:
            giris_csv = CIKTI_KLASORU / CSV_DOSYA_ADI
        
        if cikti_csv is None:
            cikti_csv = CIKTI_KLASORU / CSV_SCALED_DOSYA_ADI
        
        # CSV'yi oku
        try:
            df = pd.read_csv(giris_csv)
        except Exception as e:
            print(f"[HATA] CSV okunamadÄ±: {e}")
            return pd.DataFrame()
        
        # Ã–lÃ§eklendirilecek sÃ¼tunlarÄ± belirle (sayÄ±sal olanlar)
        kategorik_sutunlar = ['dosya_adi', 'sinif', 'tam_yol']
        sayisal_sutunlar = [col for col in df.columns if col not in kategorik_sutunlar]
        
        # Scaling seÃ§imi
        if metod == "minmax":
            scaler = MinMaxScaler()
            print(f"\nðŸ“Š MinMaxScaler: DeÄŸerleri [0, 1] aralÄ±ÄŸÄ±na Ã¶lÃ§eklendirir")
        elif metod == "robust":
            scaler = RobustScaler()
            print(f"\nðŸ“Š RobustScaler: Medyan ve IQR kullanÄ±r (aykÄ±rÄ± deÄŸerlere dayanÄ±klÄ±)")
        elif metod == "standard":
            scaler = StandardScaler()
            print(f"\nðŸ“Š StandardScaler: Z-score normalizasyonu (mean=0, std=1)")
        elif metod == "maxabs":
            scaler = MaxAbsScaler()
            print(f"\nðŸ“Š MaxAbsScaler: DeÄŸerleri [-1, 1] aralÄ±ÄŸÄ±na Ã¶lÃ§eklendirir")
        else:
            print(f"[HATA] Bilinmeyen scaling metodu: {metod}")
            print(f"       GeÃ§erli metodlar: minmax, robust, standard, maxabs")
            return df
        
        df_scaled = df.copy()
        df_scaled[sayisal_sutunlar] = scaler.fit_transform(df[sayisal_sutunlar])
        
        # Kaydet
        df_scaled.to_csv(cikti_csv, index=False, encoding='utf-8')
        print(f"\nâœ“ Ã–lÃ§eklendirilmiÅŸ CSV kaydedildi: {cikti_csv}")
        print(f"  Metod: {metod}")
        print(f"  Ä°ÅŸlenen Ã¶zellik sayÄ±sÄ±: {len(sayisal_sutunlar)}")
        
        # Scaling istatistikleri gÃ¶ster
        print(f"\nðŸ“ˆ Ã–lÃ§eklendirme sonrasÄ± deÄŸer aralÄ±klarÄ±:")
        for col in sayisal_sutunlar[:5]:  # Ä°lk 5 Ã¶zelliÄŸi gÃ¶ster
            min_val = df_scaled[col].min()
            max_val = df_scaled[col].max()
            print(f"   â€¢ {col}: [{min_val:.4f}, {max_val:.4f}]")
        if len(sayisal_sutunlar) > 5:
            print(f"   ... ve {len(sayisal_sutunlar) - 5} Ã¶zellik daha")
        
        return df_scaled
    
    def istatistik_raporu(self, csv_dosyasi: Optional[Path] = None):
        """
        CSV dosyasÄ±ndan istatistik raporu oluÅŸtur.
        
        Args:
            csv_dosyasi: CSV dosyasÄ± yolu
        """
        if csv_dosyasi is None:
            csv_dosyasi = CIKTI_KLASORU / CSV_DOSYA_ADI
        
        try:
            df = pd.read_csv(csv_dosyasi)
        except Exception as e:
            print(f"[HATA] CSV okunamadÄ±: {e}")
            return
        
        print("\n" + "="*60)
        print("VERÄ° SETÄ° Ä°STATÄ°STÄ°K RAPORU")
        print("="*60)
        
        print(f"\nToplam gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {len(df)}")
        print(f"\nSÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
        print(df['sinif'].value_counts().to_string())
        
        print(f"\n\nTemel istatistikler:")
        print(df.describe().to_string())
        
        # Eksik deÄŸerler
        eksik = df.isnull().sum()
        if eksik.sum() > 0:
            print(f"\n\nEksik deÄŸerler:")
            print(eksik[eksik > 0].to_string())
        else:
            print(f"\n\nâœ“ Eksik deÄŸer yok")
        
        print("\n" + "="*60)


def veri_boluntule(csv_dosyasi: Optional[Path] = None,
                   cikti_klasoru: Optional[Path] = None):
    """
    Veri setini eÄŸitim, doÄŸrulama ve test setlerine bÃ¶l.
    
    Makine Ã¶ÄŸrenmesinde 3 farklÄ± veri setine ihtiyaÃ§ vardÄ±r:
    
    1. EÄŸitim Seti (Train Set): ~%70
       - Model bu veriyle eÄŸitilir
       - Model, buradaki Ã¶rneklerden Ã¶ÄŸrenir
       - En bÃ¼yÃ¼k pay bu sette olmalÄ±
    
    2. DoÄŸrulama Seti (Validation Set): ~%15
       - Model eÄŸitimi sÄ±rasÄ±nda performans kontrolÃ¼
       - Hiperparametre optimizasyonu
       - Overfitting tespiti (erken durdurma - early stopping)
       - Model seÃ§imi ve karÅŸÄ±laÅŸtÄ±rma
    
    3. Test Seti (Test Set): ~%15
       - Model hiÃ§ gÃ¶rmemiÅŸ verilerle son deÄŸerlendirme
       - GerÃ§ek dÃ¼nya performansÄ±nÄ±n tahmini
       - Sadece en son deÄŸerlendirme iÃ§in kullanÄ±lÄ±r
       - YayÄ±nlanan metriklerin kaynaÄŸÄ±
    
    Stratified Splitting:
    - SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± korunur (stratify=True)
    - Her sette aynÄ± sÄ±nÄ±f oranlarÄ± olur
    - Ã–rn: EÄŸitim setinde %30 NonDemented -> test setinde de ~%30
    - Dengesiz veri setleri iÃ§in kritik Ã¶nem taÅŸÄ±r!
    
    Ã‡Ä±ktÄ± dosyalarÄ±:
    - ozellikler_egitim.csv
    - ozellikler_dogrulama.csv
    - ozellikler_test.csv
    
    Args:
        csv_dosyasi: Tam veri seti CSV dosyasÄ± (None ise varsayÄ±lan)
        cikti_klasoru: BÃ¶lÃ¼nmÃ¼ÅŸ verilerin kaydedileceÄŸi klasÃ¶r (None ise varsayÄ±lan)
    
    Returns:
        None (CSV dosyalarÄ±nÄ± kaydeder)
    """
    from sklearn.model_selection import train_test_split
    
    if csv_dosyasi is None:
        csv_dosyasi = CIKTI_KLASORU / CSV_DOSYA_ADI
    
    if cikti_klasoru is None:
        cikti_klasoru = CIKTI_KLASORU
    
    # CSV'yi oku
    try:
        df = pd.read_csv(csv_dosyasi)
    except Exception as e:
        print(f"[HATA] CSV okunamadÄ±: {e}")
        return
    
    # Etiketleri al
    y = df['etiket']
    
    # Ä°lk bÃ¶lme: eÄŸitim + (doÄŸrulama + test)
    train_df, temp_df = train_test_split(
        df, 
        test_size=(1 - EGITIM_ORANI),
        stratify=y,
        random_state=RASTGELE_TOHUM
    )
    
    # Ä°kinci bÃ¶lme: doÄŸrulama + test
    val_oran = DOGRULAMA_ORANI / (DOGRULAMA_ORANI + TEST_ORANI)
    val_df, test_df = train_test_split(
        temp_df,
        test_size=(1 - val_oran),
        stratify=temp_df['etiket'],
        random_state=RASTGELE_TOHUM
    )
    
    # Kaydet
    train_df.to_csv(cikti_klasoru / "egitim.csv", index=False)
    val_df.to_csv(cikti_klasoru / "dogrulama.csv", index=False)
    test_df.to_csv(cikti_klasoru / "test.csv", index=False)
    
    print("\nâœ“ Veri seti bÃ¶lÃ¼ndÃ¼:")
    print(f"  EÄŸitim: {len(train_df)} ({EGITIM_ORANI*100:.0f}%)")
    print(f"  DoÄŸrulama: {len(val_df)} ({DOGRULAMA_ORANI*100:.0f}%)")
    print(f"  Test: {len(test_df)} ({TEST_ORANI*100:.0f}%)")
